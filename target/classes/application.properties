# application-ollama.properties (or add to application.properties)
spring.application.name=javachatbot

# tell Spring AI to use ollama as the chat model (enabled by default)
spring.ai.model.chat=ollama

# base URL of Ollama server (include http://)
spring.ai.ollama.base-url=http://localhost:11434

# model to use in Ollama
spring.ai.ollama.chat.options.model=llama3

# optional: auto-pull strategy so Spring will download model if missing (dev only)
spring.ai.ollama.init.pull-model-strategy=when_missing
spring.ai.ollama.init.timeout=5m

